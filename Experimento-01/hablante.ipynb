{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a377913f7e492d88b8d376570c51c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/512M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrcom\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mrcom\\.cache\\huggingface\\hub\\datasets--ciempiess--ciempiess_test. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bff37c2f7a54db6aa2aacb788a39649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0001.parquet:   0%|          | 0.00/56.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8728bf0f61994610ba2e67a4dbbe75d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Cargamos el dataset CIEMPIESS\n",
    "dataset = load_dataset(\"ciempiess/ciempiess_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856b59bddc494857be898e8679666f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrcom\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mrcom\\.cache\\huggingface\\hub\\models--microsoft--wavlm-base-sv. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afc8798dfa64dedbe3d40213fd0290f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/58.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d551898eb544bb80c1552ca5f52fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargamos el modelo preentrenado de Hugging Face y el extractor de caracteristicas\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\n",
    "model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['audio_id', 'audio', 'speaker_id', 'gender', 'duration', 'normalized_text'],\n",
      "        num_rows: 3558\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accedemos al subconjunto 'test' del dataset\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "reference_speaker_id = 'M_07'\n",
    "reference_audio = test_dataset.filter(lambda x: x[\"speaker_id\"] == reference_speaker_id)[\"audio\"][0][\"array\"]\n",
    "\n",
    "# Extraer las características del audio de referencia\n",
    "reference_input = feature_extractor(reference_audio, return_tensors=\"pt\")\n",
    "reference_embedding = model(**reference_input).embeddings\n",
    "reference_embedding = torch.nn.functional.normalize(reference_embedding, dim=-1).cpu()\n",
    "\n",
    "# Función para comparar la voz del hablante con los otros hablantes\n",
    "def compare_speakers(audio1, audio2, threshold=0.86 ):\n",
    "    cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "    similarity = cosine_sim(audio1, audio2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Comparar la voz del hablante M_07 con todos los demás hablantes\n",
    "for sample in test_dataset:\n",
    "    if sample[\"speaker_id\"] != reference_speaker_id:\n",
    "        other_audio = sample[\"audio\"][\"array\"]\n",
    "        other_input = feature_extractor(other_audio, return_tensors=\"pt\")\n",
    "        other_embedding = model(**other_input).embeddings\n",
    "        other_embedding = torch.nn.functional.normalize(other_embedding, dim=-1).cpu()\n",
    "\n",
    "        if compare_speakers(reference_embedding, other_embedding):\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} es similar a la del hablante {sample['speaker_id']}.\")\n",
    "        else:\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} NO coincide con la del hablante {sample['speaker_id']}.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_01.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_05.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_07.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_04.\n",
      "La voz del hablante con ID M_07 es similar a la del hablante M_01.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_08.\n",
      "La voz del hablante con ID M_07 es similar a la del hablante M_10.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_09.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_03.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_04.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_03.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_10.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_06.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_09.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_02.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_06.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_08.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_05.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_02.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Sample rate deseado (16000 Hz)\n",
    "sampling_rate = 16000\n",
    "\n",
    "# Cargamos y filtramos el audio de referencia con el sampling_rate\n",
    "reference_speaker_id = 'M_07'\n",
    "reference_audio = test_dataset.filter(lambda x: x[\"speaker_id\"] == reference_speaker_id)[\"audio\"][0][\"array\"]\n",
    "\n",
    "# Extraer las características del audio de referencia\n",
    "reference_input = feature_extractor(reference_audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "reference_embedding = model(**reference_input).embeddings\n",
    "reference_embedding = torch.nn.functional.normalize(reference_embedding, dim=-1).cpu()\n",
    "\n",
    "# Funcion para comparar la voz del hablante con los otros hablantes\n",
    "def compare_speakers(audio1, audio2, threshold=0.86):\n",
    "    cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "    similarity = cosine_sim(audio1, audio2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Lista de los hablantes ya comparados\n",
    "compared_speakers = []\n",
    "\n",
    "# Comparamos la voz del hablante M_07 con todos los demas hablantes\n",
    "for sample in test_dataset:\n",
    "    if sample[\"speaker_id\"] != reference_speaker_id and sample[\"speaker_id\"] not in compared_speakers:\n",
    "        other_audio = sample[\"audio\"][\"array\"]\n",
    "        \n",
    "        # Extraemos las caracteristicas del otro hablante asegurando el sampling_rate\n",
    "        other_input = feature_extractor(other_audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        other_embedding = model(**other_input).embeddings\n",
    "        other_embedding = torch.nn.functional.normalize(other_embedding, dim=-1).cpu()\n",
    "\n",
    "        # Comparamos las voces\n",
    "        if compare_speakers(reference_embedding, other_embedding):\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} es similar a la del hablante {sample['speaker_id']}.\")\n",
    "        else:\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} NO coincide con la del hablante {sample['speaker_id']}.\")\n",
    "        \n",
    "        compared_speakers.append(sample[\"speaker_id\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
