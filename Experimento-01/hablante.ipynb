{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Cargamos el dataset CIEMPIESS\n",
    "dataset = load_dataset(\"ciempiess/ciempiess_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo preentrenado de Hugging Face y el extractor de caracteristicas\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\n",
    "model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['audio_id', 'audio', 'speaker_id', 'gender', 'duration', 'normalized_text'],\n",
      "        num_rows: 3558\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accedemos al subconjunto 'test' del dataset\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "reference_speaker_id = 'M_07'\n",
    "reference_audio = test_dataset.filter(lambda x: x[\"speaker_id\"] == reference_speaker_id)[\"audio\"][0][\"array\"]\n",
    "\n",
    "# Extraer las características del audio de referencia\n",
    "reference_input = feature_extractor(reference_audio, return_tensors=\"pt\")\n",
    "reference_embedding = model(**reference_input).embeddings\n",
    "reference_embedding = torch.nn.functional.normalize(reference_embedding, dim=-1).cpu()\n",
    "\n",
    "# Función para comparar la voz del hablante con los otros hablantes\n",
    "def compare_speakers(audio1, audio2, threshold=0.86 ):\n",
    "    cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "    similarity = cosine_sim(audio1, audio2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Comparar la voz del hablante M_07 con todos los demás hablantes\n",
    "for sample in test_dataset:\n",
    "    if sample[\"speaker_id\"] != reference_speaker_id:\n",
    "        other_audio = sample[\"audio\"][\"array\"]\n",
    "        other_input = feature_extractor(other_audio, return_tensors=\"pt\")\n",
    "        other_embedding = model(**other_input).embeddings\n",
    "        other_embedding = torch.nn.functional.normalize(other_embedding, dim=-1).cpu()\n",
    "\n",
    "        if compare_speakers(reference_embedding, other_embedding):\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} es similar a la del hablante {sample['speaker_id']}.\")\n",
    "        else:\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} NO coincide con la del hablante {sample['speaker_id']}.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_01.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_05.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_07.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_04.\n",
      "La voz del hablante con ID M_07 es similar a la del hablante M_01.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_08.\n",
      "La voz del hablante con ID M_07 es similar a la del hablante M_10.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_09.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_03.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_04.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_03.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_10.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_06.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_09.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_02.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_06.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_08.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_05.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_02.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Sample rate deseado (16000 Hz)\n",
    "sampling_rate = 16000\n",
    "\n",
    "# Cargamos y filtramos el audio de referencia con el sampling_rate\n",
    "reference_speaker_id = 'M_07'\n",
    "reference_audio = test_dataset.filter(lambda x: x[\"speaker_id\"] == reference_speaker_id)[\"audio\"][0][\"array\"]\n",
    "\n",
    "# Extraer las características del audio de referencia\n",
    "reference_input = feature_extractor(reference_audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "reference_embedding = model(**reference_input).embeddings\n",
    "reference_embedding = torch.nn.functional.normalize(reference_embedding, dim=-1).cpu()\n",
    "\n",
    "# Funcion para comparar la voz del hablante con los otros hablantes\n",
    "def compare_speakers(audio1, audio2, threshold=0.86):\n",
    "    cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "    similarity = cosine_sim(audio1, audio2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Lista de los hablantes ya comparados\n",
    "compared_speakers = []\n",
    "\n",
    "# Comparamos la voz del hablante M_07 con todos los demas hablantes\n",
    "for sample in test_dataset:\n",
    "    if sample[\"speaker_id\"] != reference_speaker_id and sample[\"speaker_id\"] not in compared_speakers:\n",
    "        other_audio = sample[\"audio\"][\"array\"]\n",
    "        \n",
    "        # Extraemos las caracteristicas del otro hablante asegurando el sampling_rate\n",
    "        other_input = feature_extractor(other_audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        other_embedding = model(**other_input).embeddings\n",
    "        other_embedding = torch.nn.functional.normalize(other_embedding, dim=-1).cpu()\n",
    "\n",
    "        # Comparamos las voces\n",
    "        if compare_speakers(reference_embedding, other_embedding):\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} es similar a la del hablante {sample['speaker_id']}.\")\n",
    "        else:\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} NO coincide con la del hablante {sample['speaker_id']}.\")\n",
    "        \n",
    "        compared_speakers.append(sample[\"speaker_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3412914ffc4c1a82327856063d92c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrcom\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f18240ce204e5788c88eefe880b71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0b14f5641e4de0802bc4a8381fa4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8972c591114f71ac75c55533ff6d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf339a62a884f1cbee9f106cf2b434c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837c7c2475ca433a871aca3cf3403803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c315b26d61c40d68c00ead238607712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50921b6cdc3e4a9fa0d81a9f1d9932a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación M_01 vs M_02: Similares.\n",
      "Comparación M_01 vs M_03: Similares.\n",
      "Comparación M_01 vs M_04: No similares.\n",
      "Comparación M_01 vs M_05: Similares.\n",
      "Comparación M_01 vs M_06: Similares.\n",
      "Comparación M_01 vs M_07: Similares.\n",
      "Comparación M_01 vs M_08: No similares.\n",
      "Comparación M_02 vs M_01: Similares.\n",
      "Comparación M_02 vs M_03: Similares.\n",
      "Comparación M_02 vs M_04: Similares.\n",
      "Comparación M_02 vs M_05: Similares.\n",
      "Comparación M_02 vs M_06: Similares.\n",
      "Comparación M_02 vs M_07: No similares.\n",
      "Comparación M_02 vs M_08: No similares.\n",
      "Comparación M_03 vs M_01: Similares.\n",
      "Comparación M_03 vs M_02: Similares.\n",
      "Comparación M_03 vs M_04: Similares.\n",
      "Comparación M_03 vs M_05: Similares.\n",
      "Comparación M_03 vs M_06: Similares.\n",
      "Comparación M_03 vs M_07: No similares.\n",
      "Comparación M_03 vs M_08: No similares.\n",
      "Comparación M_04 vs M_01: No similares.\n",
      "Comparación M_04 vs M_02: Similares.\n",
      "Comparación M_04 vs M_03: Similares.\n",
      "Comparación M_04 vs M_05: Similares.\n",
      "Comparación M_04 vs M_06: Similares.\n",
      "Comparación M_04 vs M_07: No similares.\n",
      "Comparación M_04 vs M_08: No similares.\n",
      "Comparación M_05 vs M_01: Similares.\n",
      "Comparación M_05 vs M_02: Similares.\n",
      "Comparación M_05 vs M_03: Similares.\n",
      "Comparación M_05 vs M_04: Similares.\n",
      "Comparación M_05 vs M_06: Similares.\n",
      "Comparación M_05 vs M_07: No similares.\n",
      "Comparación M_05 vs M_08: No similares.\n",
      "Comparación M_06 vs M_01: Similares.\n",
      "Comparación M_06 vs M_02: Similares.\n",
      "Comparación M_06 vs M_03: Similares.\n",
      "Comparación M_06 vs M_04: Similares.\n",
      "Comparación M_06 vs M_05: Similares.\n",
      "Comparación M_06 vs M_07: No similares.\n",
      "Comparación M_06 vs M_08: No similares.\n",
      "Comparación M_07 vs M_01: Similares.\n",
      "Comparación M_07 vs M_02: No similares.\n",
      "Comparación M_07 vs M_03: No similares.\n",
      "Comparación M_07 vs M_04: No similares.\n",
      "Comparación M_07 vs M_05: No similares.\n",
      "Comparación M_07 vs M_06: No similares.\n",
      "Comparación M_07 vs M_08: No similares.\n",
      "Comparación M_08 vs M_01: No similares.\n",
      "Comparación M_08 vs M_02: No similares.\n",
      "Comparación M_08 vs M_03: No similares.\n",
      "Comparación M_08 vs M_04: No similares.\n",
      "Comparación M_08 vs M_05: No similares.\n",
      "Comparación M_08 vs M_06: No similares.\n",
      "Comparación M_08 vs M_07: No similares.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Sample rate deseado (16000 Hz)\n",
    "sampling_rate = 16000\n",
    "\n",
    "# Funcion para comparar la voz del hablante con los otros hablantes\n",
    "def compare_speakers(audio1, audio2, threshold=0.86):\n",
    "    cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "    similarity = cosine_sim(audio1, audio2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Inicializamos una lista para almacenar los embeddings de los hablantes M_01 a M_09\n",
    "speakers_embeddings = {}\n",
    "speakers_ids = [f\"M_0{i}\" for i in range(1, 9)]  # Lista de IDs M_01 a M_09\n",
    "\n",
    "# Generamos los embeddings para los hablantes de referencia M_01 a M_09\n",
    "for speaker_id in speakers_ids:\n",
    "    speaker_audio = test_dataset.filter(lambda x: x[\"speaker_id\"] == speaker_id)[\"audio\"][0][\"array\"]\n",
    "    \n",
    "    # Extraer las características del audio\n",
    "    speaker_input = feature_extractor(speaker_audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "    speaker_embedding = model(**speaker_input).embeddings\n",
    "    speaker_embedding = torch.nn.functional.normalize(speaker_embedding, dim=-1).cpu()\n",
    "    \n",
    "    # Almacenar el embedding\n",
    "    speakers_embeddings[speaker_id] = speaker_embedding\n",
    "\n",
    "# Creamos una matriz de comparación vacía\n",
    "comparison_matrix = {}\n",
    "\n",
    "# Comparar cada hablante con los demás (M_01 con M_02, M_01 con M_03, etc.)\n",
    "for speaker_ref in speakers_ids:\n",
    "    comparison_matrix[speaker_ref] = {}\n",
    "    for speaker_target in speakers_ids:\n",
    "        if speaker_ref == speaker_target:\n",
    "            comparison_matrix[speaker_ref][speaker_target] = True  # La comparación con uno mismo siempre es similar\n",
    "        else:\n",
    "            # Comparamos las voces de los hablantes\n",
    "            is_similar = compare_speakers(speakers_embeddings[speaker_ref], speakers_embeddings[speaker_target])\n",
    "            comparison_matrix[speaker_ref][speaker_target] = is_similar\n",
    "            print(f\"Comparación {speaker_ref} vs {speaker_target}: {'Similares' if is_similar else 'No similares'}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
