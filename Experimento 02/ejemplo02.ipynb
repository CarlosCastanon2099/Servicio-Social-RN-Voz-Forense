{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repliación de ejemplo\n",
    "\n",
    "#Código de ejemplo de WVAM\n",
    "  \n",
    "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "#dataset ciempies\n",
    "\n",
    "dataset = load_dataset(\"ciempiess/ciempiess_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo predeterminado\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\n",
    "model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['audio_id', 'audio', 'speaker_id', 'gender', 'duration', 'normalized_text'],\n",
      "        num_rows: 3558\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accedemos al subconjunto 'test' del dataset\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ejemplo M_07</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_01.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_05.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_07.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_04.\n",
      "La voz del hablante con ID M_07 es similar a la del hablante M_01.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_08.\n",
      "La voz del hablante con ID M_07 es similar a la del hablante M_10.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_09.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_03.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_04.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_03.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_10.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_06.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_09.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_02.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_06.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_08.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante M_05.\n",
      "La voz del hablante con ID M_07 NO coincide con la del hablante F_02.\n"
     ]
    }
   ],
   "source": [
    "# Sample rate deseado (16000 Hz)\n",
    "sampling_rate = 16000\n",
    "\n",
    "# Cargamos y filtramos el audio de referencia con el sampling_rate\n",
    "reference_speaker_id = 'M_07'\n",
    "reference_audio = test_dataset.filter(lambda x: x[\"speaker_id\"] == reference_speaker_id)[\"audio\"][0][\"array\"]\n",
    "\n",
    "# Extraer las características del audio de referencia\n",
    "reference_input = feature_extractor(reference_audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "reference_embedding = model(**reference_input).embeddings\n",
    "reference_embedding = torch.nn.functional.normalize(reference_embedding, dim=-1).cpu()\n",
    "\n",
    "# Funcion para comparar la voz del hablante con los otros hablantes\n",
    "def compare_speakers(audio1, audio2, threshold=0.86):\n",
    "    cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "    similarity = cosine_sim(audio1, audio2)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Lista de los hablantes ya comparados\n",
    "compared_speakers = []\n",
    "\n",
    "# Comparamos la voz del hablante M_07 con todos los demas hablantes\n",
    "for sample in test_dataset:\n",
    "    if sample[\"speaker_id\"] != reference_speaker_id and sample[\"speaker_id\"] not in compared_speakers:\n",
    "        other_audio = sample[\"audio\"][\"array\"]\n",
    "        \n",
    "        # Extraemos las caracteristicas del otro hablante asegurando el sampling_rate\n",
    "        other_input = feature_extractor(other_audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        other_embedding = model(**other_input).embeddings\n",
    "        other_embedding = torch.nn.functional.normalize(other_embedding, dim=-1).cpu()\n",
    "\n",
    "        # Comparación\n",
    "        if compare_speakers(reference_embedding, other_embedding):\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} es similar a la del hablante {sample['speaker_id']}.\")\n",
    "        else:\n",
    "            print(f\"La voz del hablante con ID {reference_speaker_id} NO coincide con la del hablante {sample['speaker_id']}.\")\n",
    "        \n",
    "        compared_speakers.append(sample[\"speaker_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ejemplo M_01</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La voz del hablante M_01 es similar a la del hablante M_07.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_01.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_05.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_07.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_04.\n",
      "La voz del hablante M_01 es similar a la del hablante F_08.\n",
      "La voz del hablante M_01 es similar a la del hablante M_10.\n",
      "La voz del hablante M_01 es similar a la del hablante M_09.\n",
      "La voz del hablante M_01 es similar a la del hablante M_03.\n",
      "La voz del hablante M_01 NO coincide con la del hablante M_04.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_03.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_10.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_06.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_09.\n",
      "La voz del hablante M_01 es similar a la del hablante M_02.\n",
      "La voz del hablante M_01 es similar a la del hablante M_06.\n",
      "La voz del hablante M_01 NO coincide con la del hablante M_08.\n",
      "La voz del hablante M_01 es similar a la del hablante M_05.\n",
      "La voz del hablante M_01 NO coincide con la del hablante F_02.\n"
     ]
    }
   ],
   "source": [
    "# tasa de muestreo  (16 kHz)\n",
    "tasa_muestreo = 16000\n",
    "\n",
    "# hablante de referencia\n",
    "hablante_objetivo = 'M_01'\n",
    "\n",
    "# Filtramos el dataset para obtener el audio del hablante objetivo\n",
    "audio_objetivo = test_dataset.filter(lambda muestra: muestra[\"speaker_id\"] == hablante_objetivo)[\"audio\"][0][\"array\"]\n",
    "\n",
    "# Procesamos el audio de referencia para extraer los embeddings\n",
    "entrada_objetivo = feature_extractor(audio_objetivo, sampling_rate=tasa_muestreo, return_tensors=\"pt\")\n",
    "embedding_objetivo = model(**entrada_objetivo).embeddings\n",
    "embedding_objetivo_normalizado = torch.nn.functional.normalize(embedding_objetivo, dim=-1).cpu()\n",
    "\n",
    "# Función para verificar la similitud de voces\n",
    "def comparar_voces(embedding1, embedding2, umbral_similitud=0.86):\n",
    "    similitud_coseno = torch.nn.CosineSimilarity(dim=-1)\n",
    "    puntuacion_similitud = similitud_coseno(embedding1, embedding2)\n",
    "    return puntuacion_similitud >= umbral_similitud\n",
    "\n",
    "# lista de hablantes ya procesados\n",
    "hablantes_procesados = []\n",
    "\n",
    "# Recorremos el dataset para comparar el hablante M_01 con los demás\n",
    "for muestra in test_dataset:\n",
    "    hablante_actual = muestra[\"speaker_id\"]\n",
    "\n",
    "    # Comparamos solo con hablantes que no sean el objetivo y que no hayan sido procesados antes\n",
    "    if hablante_actual != hablante_objetivo and hablante_actual not in hablantes_procesados:\n",
    "        audio_actual = muestra[\"audio\"][\"array\"]\n",
    "\n",
    "        # Procesamiento el audio actual para extraer los embeddings\n",
    "        entrada_actual = feature_extractor(audio_actual, sampling_rate=tasa_muestreo, return_tensors=\"pt\")\n",
    "        embedding_actual = model(**entrada_actual).embeddings\n",
    "        embedding_actual_normalizado = torch.nn.functional.normalize(embedding_actual, dim=-1).cpu()\n",
    "\n",
    "        # Comparación de voces\n",
    "        if comparar_voces(embedding_objetivo_normalizado, embedding_actual_normalizado):\n",
    "            print(f\"La voz del hablante {hablante_objetivo} es similar a la del hablante {hablante_actual}.\")\n",
    "        else:\n",
    "            print(f\"La voz del hablante {hablante_objetivo} NO coincide con la del hablante {hablante_actual}.\")\n",
    "\n",
    "        # Marcamos al hablante actual como procesado\n",
    "        hablantes_procesados.append(hablante_actual)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
